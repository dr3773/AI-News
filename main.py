import os
import logging
import re
from html import unescape, escape
from time import mktime
from datetime import time, datetime
from zoneinfo import ZoneInfo
from typing import List, Dict, Set

import feedparser
from telegram import Update
from telegram.constants import ParseMode
from telegram.ext import (
    Application,
    CommandHandler,
    ContextTypes,
)

# ------------------- –ù–ê–°–¢–†–û–ô–ö–ò -------------------

TOKEN = os.environ.get("BOT_TOKEN") or os.environ.get("TOKEN")
if not TOKEN:
    raise RuntimeError("–ù–µ –∑–∞–¥–∞–Ω —Ç–æ–∫–µ–Ω –±–æ—Ç–∞. –£–∫–∞–∂–∏ BOT_TOKEN –∏–ª–∏ TOKEN –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.")

CHANNEL_ID = os.environ.get("CHANNEL_ID")
if not CHANNEL_ID:
    raise RuntimeError("–ù–µ –∑–∞–¥–∞–Ω CHANNEL_ID ‚Äî id –∏–ª–∏ @username –∫–∞–Ω–∞–ª–∞.")

ADMIN_CHAT_ID = os.environ.get("ADMIN_CHAT_ID", "").strip() or None

# –∏–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π (—Å–µ–∫—É–Ω–¥—ã)
NEWS_INTERVAL = int(os.environ.get("NEWS_INTERVAL", "1800"))  # 30 –º–∏–Ω—É—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

# —á–∞—Å–æ–≤–æ–π –ø–æ—è—Å (–î—É—à–∞–Ω–±–µ)
TZ = ZoneInfo(os.environ.get("TIMEZONE", "Asia/Dushanbe"))

SENT_URLS_FILE = "sent_urls.json"

FEEDS: List[str] = [
    "https://nplus1.ru/rss",
    "https://habr.com/ru/rss/hub/machine_learning/all/",
    "https://habr.com/ru/rss/hub/artificial_intelligence/all/",
    "https://ai.googleblog.com/feeds/posts/default?alt=rss",
    "https://openai.com/blog/rss.xml",
]

TODAY_NEWS: List[Dict] = []
SENT_URLS: Set[str] = set()

# ------------------- –õ–û–ì–ò -------------------

logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger("ai-news-bot")

# ------------------- –•–ï–õ–ü–ï–†–´ -------------------


def clean_html(text: str) -> str:
    """–£–±–∏—Ä–∞–µ–º HTML-—Ç–µ–≥–∏ –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã."""
    if not text:
        return ""
    text = unescape(text)
    text = re.sub(r"<.*?>", "", text)
    return text.strip()


def load_sent_urls() -> None:
    """–ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞."""
    import json

    global SENT_URLS
    if not os.path.exists(SENT_URLS_FILE):
        SENT_URLS = set()
        return

    try:
        with open(SENT_URLS_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        SENT_URLS = set(data)
        logger.info("–ó–∞–≥—Ä—É–∂–µ–Ω–æ %d –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫.", len(SENT_URLS))
    except Exception as e:
        logger.exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å %s: %s", SENT_URLS_FILE, e)
        SENT_URLS = set()


def save_sent_urls() -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ñ–∞–π–ª."""
    import json

    try:
        with open(SENT_URLS_FILE, "w", encoding="utf-8") as f:
            json.dump(sorted(SENT_URLS), f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.exception("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å %s: %s", SENT_URLS_FILE, e)


def parse_entry(feed_title: str, entry) -> Dict:
    """–ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –∑–∞–ø–∏—Å—å RSS –≤ —Å–ª–æ–≤–∞—Ä—å."""
    title = entry.get("title", "").strip()
    summary = entry.get("summary", "") or entry.get("description", "")
    link = entry.get("link", "")

    published_parsed = entry.get("published_parsed") or entry.get("updated_parsed")
    if published_parsed:
        dt = datetime.fromtimestamp(mktime(published_parsed), tz=TZ)
    else:
        dt = datetime.now(TZ)

    image_url = ""
    content = ""
    if "content" in entry and entry.content:
        content = " ".join([c.value for c in entry.content if hasattr(c, "value")])
    else:
        content = summary or ""

    m = re.search(r'src="([^"]+)"', content)
    if m:
        image_url = m.group(1)

    return {
        "title": title or feed_title,
        "summary": summary,
        "url": link,
        "image": image_url,
        "date": dt.date().isoformat(),
        "feed": feed_title,
    }


def fetch_news() -> List[Dict]:
    """–ß–∏—Ç–∞–µ–º –≤—Å–µ RSS-–ª–µ–Ω—Ç—ã –∏ —Å–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏."""
    items: List[Dict] = []

    for feed_url in FEEDS:
        try:
            parsed = feedparser.parse(feed_url)
            feed_title = parsed.feed.get("title", "–ò—Å—Ç–æ—á–Ω–∏–∫")
            for entry in parsed.entries:
                link = entry.get("link")
                if not link or link in SENT_URLS:
                    continue

                item = parse_entry(feed_title, entry)
                items.append(item)
        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ %s: %s", feed_url, e)

    # –Ω–æ–≤—ã–µ —Å–≤–µ—Ä—Ö—É
    items.sort(key=lambda x: x["date"], reverse=True)
    return items


async def notify_admin(context: ContextTypes.DEFAULT_TYPE, text: str) -> None:
    """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–ª—É–∂–µ–±–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –∞–¥–º–∏–Ω—É (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)."""
    if not ADMIN_CHAT_ID:
        return
    try:
        await context.bot.send_message(chat_id=ADMIN_CHAT_ID, text=f"‚ö†Ô∏è {text}")
    except Exception:
        logger.exception("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –∞–¥–º–∏–Ω—É.")


def build_body_text(title: str, summary: str) -> str:
    """
    –ö–æ—Ä–æ—Ç–∫–∏–π –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏:
    - —á–∏—Å—Ç–∏–º —Ç–µ–≥–∏;
    - –µ—Å–ª–∏ –µ—Å—Ç—å –≤–Ω—è—Ç–Ω—ã–π summary (–Ω–µ –¥—É–±–ª–∏—Ä—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ;
    - –∏–Ω–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É.
    """
    title_clean = clean_html(title)
    summary_clean = clean_html(summary)

    if summary_clean and summary_clean.lower() != title_clean.lower():
        return summary_clean
    else:
        return ""


def build_post_text(item: Dict) -> str:
    """
    –ò—Ç–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –ø–æ—Å—Ç–∞:
    üß† <–∂–∏—Ä–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫>

    <–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ, –µ—Å–ª–∏ –µ—Å—Ç—å>

    üîó –ò—Å—Ç–æ—á–Ω–∏–∫
    """
    raw_title = item.get("title", "") or ""
    raw_summary = item.get("summary", "") or ""
    url = item.get("url", "") or ""

    title_clean = clean_html(raw_title)
    body = build_body_text(raw_title, raw_summary)

    safe_title = escape(title_clean)
    safe_body = escape(body) if body else ""
    safe_url = escape(url, quote=True) if url else ""

    parts: List[str] = []
    parts.append(f"üß† <b>{safe_title}</b>")

    if safe_body:
        parts.append("")
        if len(safe_body) > 3500:
            safe_body = safe_body[:3490] + "‚Ä¶"
        parts.append(safe_body)

    if safe_url:
        parts.append("")
        parts.append(f'üîó <a href="{safe_url}">–ò—Å—Ç–æ—á–Ω–∏–∫</a>')

    return "\n".join(parts)


# ------------------- JOBS -------------------


async def periodic_news_job(context: ContextTypes.DEFAULT_TYPE) -> None:
    """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –Ω–æ–≤—ã—Ö –≤ –∫–∞–Ω–∞–ª."""
    logger.info("–ó–∞–ø—É—Å–∫ periodic_news_job")
    try:
        new_items = fetch_news()
        if not new_items:
            logger.info("–ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç.")
            return

        for item in new_items:
            url = item["url"]
            if not url or url in SENT_URLS:
                continue

            text = build_post_text(item)

            try:
                if item.get("image"):
                    await context.bot.send_photo(
                        chat_id=CHANNEL_ID,
                        photo=item["image"],
                        caption=text,
                        parse_mode=ParseMode.HTML,
                    )
                else:
                    await context.bot.send_message(
                        chat_id=CHANNEL_ID,
                        text=text,
                        parse_mode=ParseMode.HTML,
                        disable_web_page_preview=False,
                    )

                SENT_URLS.add(url)
                save_sent_urls()

                TODAY_NEWS.append(
                    {
                        "date": item["date"],
                        "title": item["title"],
                        "url": item["url"],
                    }
                )

            except Exception as e:
                logger.exception("–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: %s", e)
                await notify_admin(context, f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")

    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ –≤ periodic_news_job: %s", e)
        await notify_admin(context, f"–û—à–∏–±–∫–∞ –≤ periodic_news_job: {e}")


async def daily_digest_job(context: ContextTypes.DEFAULT_TYPE) -> None:
    """–í–µ—á–µ—Ä–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç: –æ–¥–∏–Ω —Ä–∞–∑ –≤ –¥–µ–Ω—å –≤ 21:00 –ø–æ –î—É—à–∞–Ω–±–µ."""
    try:
        today_str = datetime.now(TZ).date().isoformat()
        today_items = [n for n in TODAY_NEWS if n["date"] == today_str]

        if not today_items:
            logger.info("–ó–∞ —Å–µ–≥–æ–¥–Ω—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç ‚Äî –¥–∞–π–¥–∂–µ—Å—Ç –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º.")
            return

        lines = ["üåô <b>–í–µ—á–µ—Ä–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –ò–ò</b>", ""]
        for i, item in enumerate(today_items, start=1):
            safe_title = escape(clean_html(item["title"]))
            safe_url = escape(item["url"], quote=True)
            lines.append(f'{i}. <a href="{safe_url}">{safe_title}</a>')

        text = "\n".join(lines)

        await context.bot.send_message(
            chat_id=CHANNEL_ID,
            text=text,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=False,
        )

        TODAY_NEWS.clear()

    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ –≤ daily_digest_job: %s", e)
        await notify_admin(context, f"–û—à–∏–±–∫–∞ –≤ daily_digest_job: {e}")


# ------------------- HANDLERS -------------------


async def start_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–ö–æ–º–∞–Ω–¥–∞ /start –≤ –ª–∏—á–∫–µ —Å –±–æ—Ç–æ–º."""
    if update.effective_chat is None:
        return

    await update.effective_chat.send_message(
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –∫–∞–Ω–∞–ª–∞ —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ –æ–± –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ.\n\n"
        "–ß—Ç–æ —è –¥–µ–ª–∞—é:\n"
        "‚Ä¢ –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–Ω—è –ø—É–±–ª–∏–∫—É—é —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–± –ò–ò –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤;\n"
        "‚Ä¢ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –æ—Ñ–æ—Ä–º–ª—è—é –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ;\n"
        "‚Ä¢ –≤ 21:00 –ø–æ –î—É—à–∞–Ω–±–µ –æ—Ç–ø—Ä–∞–≤–ª—è—é –≤–µ—á–µ—Ä–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –∑–∞ –¥–µ–Ω—å."
    )


# ------------------- MAIN -------------------


def main() -> None:
    logger.info("–ó–∞–ø—É—Å–∫ ai-news-bot")

    load_sent_urls()

    app = Application.builder().token(TOKEN).build()

    app.add_handler(CommandHandler("start", start_handler))

    job_queue = app.job_queue
    if job_queue is None:
        raise RuntimeError(
            "JobQueue –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –í requirements.txt –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å—Ç—Ä–æ–∫–∞ "
            "'python-telegram-bot[job-queue]==21.6'"
        )

    # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
    job_queue.run_repeating(
        periodic_news_job,
        interval=NEWS_INTERVAL,
        first=30,  # —á–µ—Ä–µ–∑ 30 —Å–µ–∫—É–Ω–¥ –ø–æ—Å–ª–µ —Å—Ç–∞—Ä—Ç–∞
        name="periodic_news",
    )

    # –í–µ—á–µ—Ä–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –≤ 21:00
    job_queue.run_daily(
        daily_digest_job,
        time=time(21, 0, tzinfo=TZ),
        name="daily_digest",
    )

    # –í–ê–ñ–ù–û: –±–µ–∑ asyncio.run, –±–µ–∑ —Å–≤–æ–∏—Ö event loop
    app.run_polling(allowed_updates=["message"])


if __name__ == "__main__":
    main()
