import os
import logging
import re
from html import unescape, escape
from time import mktime
from datetime import time
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import List, Dict, Set

import feedparser
from telegram import Update
from telegram.constants import ParseMode
from telegram.ext import (
    Application,
    CommandHandler,
    ContextTypes,
)

# ------------------- –õ–û–ì–ò -------------------

logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger("ai-news-bot")

# ------------------- –ù–ê–°–¢–†–û–ô–ö–ò -------------------

TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID_ENV = os.getenv("CHANNEL_ID")
ADMIN_ID_ENV = os.getenv("ADMIN_ID")

if not TOKEN:
    raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω TELEGRAM_BOT_TOKEN –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
if not CHANNEL_ID_ENV:
    raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω CHANNEL_ID –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")

CHANNEL_ID = int(CHANNEL_ID_ENV)
ADMIN_ID = int(ADMIN_ID_ENV) if ADMIN_ID_ENV else None

TZ = ZoneInfo("Asia/Dushanbe")

# Google News –ø–æ –ò–ò –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞—Ö
RSS_FEEDS = [
    "https://news.google.com/rss/search?q=–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π+–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç&hl=ru&gl=RU&ceid=RU:ru",
    "https://news.google.com/rss/search?q=–Ω–µ–π—Ä–æ—Å–µ—Ç–∏&hl=ru&gl=RU&ceid=RU:ru",
    "https://news.google.com/rss/search?q=–º–∞—à–∏–Ω–Ω–æ–µ+–æ–±—É—á–µ–Ω–∏–µ&hl=ru&gl=RU&ceid=RU:ru",
    "https://news.google.com/rss/search?q=–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π+–ò–ò&hl=ru&gl=RU&ceid=RU:ru",
    "https://news.google.com/rss/search?q=artificial+intelligence&hl=en&gl=US&ceid=US:en",
]

# –§–∞–π–ª, –∫—É–¥–∞ –ø–∏—à–µ–º —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
SENT_URLS_FILE = "sent_urls.txt"

# –í –ø–∞–º—è—Ç–∏ ‚Äì —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –∑–∞ —Ç–µ–∫—É—â–∏–π –∑–∞–ø—É—Å–∫
SENT_URLS: Set[str] = set()

# –î–ª—è –≤–µ—á–µ—Ä–Ω–µ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞
TODAY_NEWS: List[Dict[str, str]] = []

# –ß–µ—Ä–µ–∑ —Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –Ω–æ–≤–æ—Å—Ç–∏
NEWS_INTERVAL = 45 * 60  # 45 –º–∏–Ω—É—Ç

_html_tag_re = re.compile(r"<[^>]+>")
_cyr_re = re.compile(r"[–ê-–Ø–∞-—è–Å—ë]")


# ------------------- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï -------------------

def load_sent_urls() -> None:
    global SENT_URLS
    if not os.path.exists(SENT_URLS_FILE):
        SENT_URLS = set()
        return
    try:
        with open(SENT_URLS_FILE, "r", encoding="utf-8") as f:
            SENT_URLS = {line.strip() for line in f if line.strip()}
    except Exception as e:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: %s", e)
        SENT_URLS = set()


def save_sent_urls() -> None:
    try:
        with open(SENT_URLS_FILE, "w", encoding="utf-8") as f:
            for url in sorted(SENT_URLS):
                f.write(url + "\n")
    except Exception as e:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–ø–∏—Å–æ–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: %s", e)


def clean_html(text: str | None) -> str:
    if not text:
        return ""
    text = unescape(text)
    text = _html_tag_re.sub(" ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def extract_image(entry) -> str | None:
    media = getattr(entry, "media_content", None)
    if media and isinstance(media, list):
        url = media[0].get("url")
        if url:
            return url

    links = getattr(entry, "links", [])
    for l in links:
        if l.get("type", "").startswith("image/") and l.get("href"):
            return l["href"]

    return None


def fetch_new_articles(limit: int = 5) -> List[Dict]:
    """–°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ (–∫–æ—Ç–æ—Ä—ã—Ö –µ—â—ë –Ω–µ –±—ã–ª–æ –≤ SENT_URLS)."""
    items: List[Dict] = []

    for feed_url in RSS_FEEDS:
        try:
            parsed = feedparser.parse(feed_url)
        except Exception as e:
            logger.warning("–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS %s: %s", feed_url, e)
            continue

        feed_title = parsed.feed.get("title", "–ù–æ–≤–æ—Å—Ç–∏ –ò–ò")

        for entry in parsed.entries:
            link = entry.get("link")
            if not link or link in SENT_URLS:
                continue

            title = clean_html(entry.get("title", ""))
            summary = clean_html(entry.get("summary") or entry.get("description") or "")

            if not title and not summary:
                continue

            published_parsed = entry.get("published_parsed") or entry.get("updated_parsed")
            ts = mktime(published_parsed) if published_parsed else 0

            items.append(
                {
                    "title": title,
                    "summary": summary,
                    "url": link,
                    "source": feed_title,
                    "image": extract_image(entry),
                    "ts": ts,
                }
            )

    if not items:
        return []

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –±–æ–ª–µ–µ –Ω–æ–≤—ã–µ
    items.sort(key=lambda x: x["ts"], reverse=True)

    # –û—Ç–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ N –Ω–æ–≤—ã—Ö –∏ –æ–±–Ω–æ–≤–ª—è–µ–º SENT_URLS
    result: List[Dict] = []
    for item in items:
        if len(result) >= limit:
            break
        if item["url"] in SENT_URLS:
            continue
        SENT_URLS.add(item["url"])
        result.append(item)

    save_sent_urls()
    return result


import random
from html import escape

def build_body_text(title: str, summary: str) -> str:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –±–µ–∑ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤.
    """

    title_clean = title.strip()
    summary_clean = summary.strip()

    # –ï—Å–ª–∏ summary –Ω–µ—Ç –∏–ª–∏ –æ–Ω —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    if not summary_clean or summary_clean.lower() == title_clean.lower():
        base = title_clean
    else:
        base = summary_clean

    lines = []

    # üîπ 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫
    lines.append(f"üß† <b>{escape(title_clean)}</b>")
    lines.append("")

    # üîπ 2. –û—Å–Ω–æ–≤–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
    if summary_clean and summary_clean.lower() != title_clean.lower():
        lines.append(escape(summary_clean))
        lines.append("")
    else:
        lines.append("–ö—Ä–∞—Ç–∫–æ –æ –∫–ª—é—á–µ–≤–æ–º —Å–æ–±—ã—Ç–∏–∏ –≤ —Å—Ñ–µ—Ä–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞:")
        lines.append(escape(title_clean))
        lines.append("")

    # üîπ 3. –°–ª—É—á–∞–π–Ω—ã–π –∑–∞–≤–µ—Ä—à–∏—Ç–µ–ª—å–Ω—ã–π –∞–±–∑–∞—Ü
    endings = [
        "–≠—Ç–æ —Å–æ–±—ã—Ç–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫—É–¥–∞ –¥–≤–∏–∂–µ—Ç—Å—è –∏–Ω–¥—É—Å—Ç—Ä–∏—è –ò–ò.",
        "–ù–æ–≤–æ—Å—Ç—å –æ—Ç—Ä–∞–∂–∞–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.",
        "–ù–∞ —Ç–∞–∫–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–æ–∏—Ç –æ–±—Ä–∞—â–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ ‚Äî –æ–Ω–∏ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –±—É–¥—É—â–µ–µ —Ä—ã–Ω–∫–∞.",
        "–≠—Ç–æ —Ö–æ—Ä–æ—à–∏–π –æ—Ä–∏–µ–Ω—Ç–∏—Ä –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –∏ –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ –≤ —Å—Ñ–µ—Ä–µ –ò–ò.",
        "–¢–∞–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–º–æ–≥–∞—é—Ç –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞—Ç—å –¥–∏–Ω–∞–º–∏–∫—É –ò–ò-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π.",
    ]

    lines.append(random.choice(endings))

    return "\n".join(lines)

def build_post_text(item: Dict) -> str:
    """
    –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ—Å—Ç–∞ –≤ HTML-—Ñ–æ—Ä–º–∞—Ç–µ:
    üß† <–∂–∏—Ä–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫>
    <—Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏>
    ‚ûú –ò—Å—Ç–æ—á–Ω–∏–∫ (–∫–ª–∏–∫–∞–±–µ–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞)
    """
    title = item["title"]
    summary = item["summary"]
    url = item["url"]

    body = build_body_text(title, summary)

    safe_title = escape(title)
    safe_body = escape(body)
    safe_url = escape(url, quote=True)

    # –æ–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª–∏–Ω—É, —á—Ç–æ–±—ã –¢–µ–ª–µ–≥—Ä–∞–º –Ω–µ —Ä—É–≥–∞–ª—Å—è
    if len(safe_body) > 3500:
        safe_body = safe_body[:3490] + "‚Ä¶"

    parts = [
        f"üß† <b>{safe_title}</b>",
        "",
        safe_body,
        "",
        f'<a href="{safe_url}">‚ûú –ò—Å—Ç–æ—á–Ω–∏–∫</a>',
    ]
    return "\n".join(parts)


async def notify_admin(context: ContextTypes.DEFAULT_TYPE, text: str) -> None:
    if ADMIN_ID is None:
        return
    try:
        await context.bot.send_message(chat_id=ADMIN_ID, text=f"‚ö†Ô∏è AI News: {text}")
    except Exception:
        logger.exception("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –∞–¥–º–∏–Ω—É")


# ------------------- JOBS -------------------

async def periodic_news_job(context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞: –∑–∞–±—Ä–∞—Ç—å –Ω–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏ –∑–∞–ø–æ—Å—Ç–∏—Ç—å –≤ –∫–∞–Ω–∞–ª.
    """
    try:
        logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π‚Ä¶")
        items = fetch_new_articles(limit=3)
        if not items:
            logger.info("–ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
            return

        today_str = datetime.now(TZ).date().isoformat()

        for item in items:
            text = build_post_text(item)

            # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±—É—Ñ–µ—Ä –¥–ª—è –¥–∞–π–¥–∂–µ—Å—Ç–∞
            TODAY_NEWS.append(
                {
                    "title": item["title"],
                    "url": item["url"],
                    "date": today_str,
                }
            )

            try:
                if item.get("image"):
                    await context.bot.send_photo(
                        chat_id=CHANNEL_ID,
                        photo=item["image"],
                        caption=text,
                        parse_mode=ParseMode.HTML,
                    )
                else:
                    await context.bot.send_message(
                        chat_id=CHANNEL_ID,
                        text=text,
                        parse_mode=ParseMode.HTML,
                        disable_web_page_preview=False,
                    )
            except Exception as e:
                logger.exception("–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: %s", e)
                await notify_admin(context, f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")

    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ –≤ periodic_news_job: %s", e)
        await notify_admin(context, f"–û—à–∏–±–∫–∞ –≤ periodic_news_job: {e}")


async def daily_digest_job(context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    –í–µ—á–µ—Ä–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç: –æ–¥–∏–Ω —Ä–∞–∑ –≤ –¥–µ–Ω—å –≤ 21:00 –ø–æ –î—É—à–∞–Ω–±–µ.
    """
    try:
        today_str = datetime.now(TZ).date().isoformat()
        today_items = [n for n in TODAY_NEWS if n["date"] == today_str]

        if not today_items:
            logger.info("–ó–∞ —Å–µ–≥–æ–¥–Ω—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç ‚Äî –¥–∞–π–¥–∂–µ—Å—Ç –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º.")
            return

        lines = ["üåô <b>–í–µ—á–µ—Ä–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –ò–ò</b>", ""]
        for i, item in enumerate(today_items, start=1):
            safe_title = escape(item["title"])
            safe_url = escape(item["url"], quote=True)
            lines.append(f'{i}. <a href="{safe_url}">{safe_title}</a>')

        text = "\n".join(lines)

        await context.bot.send_message(
            chat_id=CHANNEL_ID,
            text=text,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=False,
        )

        # –æ—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä –∑–∞ –¥–µ–Ω—å
        TODAY_NEWS.clear()

    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ –≤ daily_digest_job: %s", e)
        await notify_admin(context, f"–û—à–∏–±–∫–∞ –≤ daily_digest_job: {e}")


# ------------------- HANDLERS -------------------

async def start_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–ö–æ–º–∞–Ω–¥–∞ /start –≤ –ª–∏—á–∫–µ —Å –±–æ—Ç–æ–º."""
    if update.effective_chat is None:
        return

    await update.effective_chat.send_message(
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è –∫–∞–Ω–∞–ª–∞ —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ –æ–± –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ.\n\n"
        "‚öôÔ∏è –ß—Ç–æ —è –¥–µ–ª–∞—é:\n"
        "‚Ä¢ –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–Ω—è –ø—É–±–ª–∏–∫—É—é —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–± –ò–ò;\n"
        "‚Ä¢ –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –¥–∞—é –∫–æ—Ä–æ—Ç–∫–æ–µ, –Ω–æ –ø–æ–Ω—è—Ç–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ;\n"
        "‚Ä¢ –≤ 21:00 –ø–æ –î—É—à–∞–Ω–±–µ –æ—Ç–ø—Ä–∞–≤–ª—è—é –≤–µ—á–µ—Ä–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –∑–∞ –¥–µ–Ω—å."
    )


# ------------------- MAIN -------------------

def main() -> None:
    logger.info("–ó–∞–ø—É—Å–∫ ai-news-bot")

    load_sent_urls()

    app = Application.builder().token(TOKEN).build()

    # –ö–æ–º–∞–Ω–¥–∞ /start
    app.add_handler(CommandHandler("start", start_handler))

    # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ (JobQueue)
    job_queue = app.job_queue
    if job_queue is None:
        raise RuntimeError(
            "JobQueue –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –í requirements.txt –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å—Ç—Ä–æ–∫–∞ "
            "'python-telegram-bot[job-queue]==21.6'"
        )

    # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
    job_queue.run_repeating(
        periodic_news_job,
        interval=NEWS_INTERVAL,
        first=30,   # —á–µ—Ä–µ–∑ 30 —Å–µ–∫—É–Ω–¥ –ø–æ—Å–ª–µ —Å—Ç–∞—Ä—Ç–∞
        name="periodic_news",
    )

    # –í–µ—á–µ—Ä–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –≤ 21:00
    job_queue.run_daily(
        daily_digest_job,
        time=time(21, 0, tzinfo=TZ),
        name="daily_digest",
    )

    # –í–ê–ñ–ù–û: –Ω–∏–∫–∞–∫–∏—Ö asyncio.run, –Ω–∏–∫–∞–∫–∏—Ö —Ä—É—á–Ω—ã—Ö event loop
    app.run_polling(allowed_updates=["message"])


if __name__ == "__main__":
    main()
